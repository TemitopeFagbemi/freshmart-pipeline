{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170a785-2a02-4c63-9e0e-37c5bc3b3e4b",
   "metadata": {},
   "source": [
    "# FreshMart Data Pipeline\n",
    "\n",
    "A simple Retail Data Pipeline built for FreshMart that handles product data ingestion, transformation, and storage, with full version control managed in GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "##  Overview\n",
    "\n",
    "This repository contains a lightweight ETL pipeline implemented in Python and Pandas, with data stored in PostgreSQL. It’s designed for simplicity and reliability—ideal for early-stage data engineering initiatives.\n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "1. **Data Ingestion**  \n",
    "   - Loads product data from a local CSV file (`freshmart_products_cstone.csv`) into a Pandas DataFrame.\n",
    "\n",
    "2. **Data Transformation**  \n",
    "   - Displays the first few rows for exploratory analysis.\n",
    "   - Cleans data: removes duplicates, fills null values, processes columns, strips numeric artifacts from product names.\n",
    "   - Computes stock value per product (`StockValue = Price × StockQuantity`).\n",
    "   - Performs aggregation by `Category`, computing average `Price` and total `StockQuantity`.\n",
    "\n",
    "3. **Data Storage**  \n",
    "   - Connects to a PostgreSQL database using `psycopg2`.\n",
    "   - Creates the `Products` table (with `SERIAL` primary key, `VARCHAR` fields, appropriately typed numeric columns).\n",
    "   - Inserts cleaned data from the DataFrame safely, within transaction blocks and rollback on failures.\n",
    "   - Supports optional bulk insertion via SQLAlchemy (recommended for scaling).\n",
    "\n",
    "---\n",
    "\n",
    "##  Project Structure\n",
    "\n",
    "freshmart_project/\n",
    "├── freshmart_products_cstone.csv ← Raw product data (15–20 rows sample)\n",
    "├── pipeline.ipynb ← Jupyter notebook with ETL steps and analysis\n",
    "├── etl_script.py ← Standalone Python script for the pipeline (optional)\n",
    "├── README.md ← Project documentation (this file)\n",
    "├── .gitignore ← Ignore config (e.g., __pycache__, secrets)\n",
    "└── .gitattributes ← Normalize line endings across OS\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.x  \n",
    "- Libraries: `pandas`, `psycopg2`, `sqlalchemy` (optional, for bulk load)  \n",
    "- PostgreSQL server installed and accessible (e.g., version 17 recommended)\n",
    "\n",
    "### Usage\n",
    "\n",
    "1. Clone the repo:\n",
    "   ```bash\n",
    "   git clone https://github.com/TemitopeFagbemi/freshmart-pipeline.git\n",
    "   cd freshmart-pipeline\n",
    "\n",
    "Prepare PostgreSQL:\n",
    "\n",
    "Ensure database freshmart_db exists; if not, create it via psql or Python script.\n",
    "\n",
    "Confirm your PostgreSQL server is running and accessible.\n",
    "\n",
    "Update connection settings in pipeline.ipynb or etl_script.py:\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    dbname=\"freshmart_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YOUR_PASSWORD\"\n",
    ")\n",
    "\n",
    "4 Run pipeline:\n",
    "\n",
    "Notebook: Open pipeline.ipynb, run all cells.\n",
    "\n",
    "Script (if available):\n",
    "\n",
    "python etl_script.py\n",
    "\n",
    "5 (Optional) Visualize results with Pandas or your preferred plotting tool.\n",
    "\n",
    "Key Features & Safeguards\n",
    "\n",
    "Data Cleaning: Strips digits from product names and handles empty/missing data gracefully.\n",
    "\n",
    "Calculated Fields: Automatically computes StockValue for inventory insights.\n",
    "\n",
    "Aggregation: Visual summary of products by Category.\n",
    "\n",
    "Safe Insertion: Uses SQL transactions (COMMIT & ROLLBACK) to ensure database integrity.\n",
    "\n",
    "Git-Friendly: Includes .gitattributes for consistent line endings, .gitignore to exclude non-essential files.\n",
    "\n",
    "Future Enhancements\n",
    "\n",
    "Use sqlalchemy + df.to_sql() for faster bulk loading.\n",
    "\n",
    "Add unit tests for transformation logic.\n",
    "\n",
    "Containerize the pipeline using Docker or incorporate task orchestration with Airflow.\n",
    "\n",
    "Expand pipeline to handle multiple data sources and downstream analytics.\n",
    "\n",
    "Contributing\n",
    "\n",
    "1 Fork the repo\n",
    "\n",
    "2 Create a branch (git checkout -b feature/your-feature)\n",
    "\n",
    "3 Commit your changes (git commit -m \"Add feature\")\n",
    "\n",
    "4 Push to your branch (git push origin feature/your-feature)\n",
    "\n",
    "5 Submit a Pull Request!\n",
    "\n",
    "Contact\n",
    "\n",
    "For questions or feedback, reach out to Temitope Fagbemi (GitHub: [TemitopeFagbemi]) via GitHub Discussions or issue tracker here.\n",
    "\n",
    "Thank you for reviewing the FreshMart pipeline—here’s to clean code and reliable data!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Why this works:\n",
    "This README clearly explains:\n",
    "- **What the pipeline does**: ingestion, cleaning, calculation, aggregation, storage.\n",
    "- **How to run it**: clones, setup, running steps.\n",
    "- **Best practices**: transaction safe insertion, cleaning steps, Git version control.\n",
    "- **Extensibility**: next steps and improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24273cce-f3ab-48db-931d-362ca11fa88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
